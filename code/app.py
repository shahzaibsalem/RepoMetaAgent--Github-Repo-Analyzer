# main.py

import os
import json
from typing import Dict, Any, List, TypedDict, Union
from langgraph.graph import StateGraph, END # <-- Corrected line
# If you need to type hint CompiledGraph, you'd typically import it from langgraph directly, 
# but often it's simplest to use 'Any' or the imported type from the main executable.

# --- 1. Import the Graph Builders from their respective locations ---
# NOTE: Adjust these relative imports based on your exact file structure.


from Agents.RepoAnalyzerAgent.RepoAnalyzer import build_analyzer_graph
from Agents.MetaDataAgent.Graph.graph import build_tag_generation_graph


# --- 2. Define the Complete, Unified State ---
# The state must include ALL fields used by BOTH graphs.
# Assuming a full state from our previous steps (including union_list, etc.)
class UnifiedAnalysisState(TypedDict):
    repo_url: str
    files: List[Dict]
    summaries: Dict[str, str]
    missing_docs: List[str]
    
    # Keyword fields (used by Tag Generation Graph)
    content_text: str 
    regex_keywords: List[str]
    spacy_keywords: List[str]
    gazetteer_keywords: List[str]
    llm_keywords: List[str]
    union_list: List[str]
    
    keywords: List[str] # Final keywords produced by the Tag Generation Graph
    final_output: Dict[str, Union[str, List, Dict]]

# --- 3. Build and Execute the Assembly Line ---

def run_assembly_line_analysis(repo_url: str) -> Dict[str, Any]:
    """
    Chains the execution of the Repo Analyzer Graph and the Tag Generation Graph.
    """
    print(f"--- üè≠ Starting Assembly Line for: {repo_url} ---")
    
    # 3a. Build Agent 1: Repo Analyzer (Initial Fetch/Check/Summarize)
    print("1. Compiling Repo Analyzer Graph...")
    repo_analyzer_app: CompiledGraph = build_analyzer_graph()
    
    # 3b. Build Agent 2: Tag Generation (Parallel Extraction/Curation)
    print("2. Compiling Tag Generation Graph...")
    tag_generator_app: CompiledGraph = build_tag_generation_graph()

    # Initial input state for the first agent
    initial_state: Dict[str, Any] = {"repo_url": repo_url}

    # ----------------------------------------------------------------
    # STAGE 1: Run Repo Analyzer (Fetch, Check Docs, Summarize, etc.)
    # ----------------------------------------------------------------
    print("\nSTAGE 1: Running Repo Analyzer...")
    
    # We run the graph and get the final state dictionary
    # NOTE: The provided code block's 'build_analyzer_graph' is missing 
    # the aggregation of 'content_text', so you must ensure that node is 
    # updated to provide it.
    
    # Since the original Agent 1 uses a simple sequential flow (Fetcher -> Checker -> simple Extractor),
    # we'll run it to the end.
    
    # The output of this graph is a dictionary containing all state fields.
    stage1_output: UnifiedAnalysisState = repo_analyzer_app.invoke(initial_state)

    print(f"‚úÖ Stage 1 Complete. Files fetched, Docs checked, Summaries created.")
    
    # --- State Transformation / Transfer ---
    # The state from Stage 1 is exactly what Stage 2 needs.
    # We use the output of Stage 1 as the input for Stage 2.
    
    # The Tag Generation Graph needs: content_text, files (for context), etc.
    stage2_input: Dict[str, Any] = stage1_output 
    
    # ----------------------------------------------------------------
    # STAGE 2: Run Tag Generator (Parallel Extractors -> Union -> Selector)
    # ----------------------------------------------------------------
    print("\nSTAGE 2: Running Tag Generator...")
    
    # Invoke the second graph using the final state from the first graph
    # The output will contain the final 'keywords' list.
    stage2_output: UnifiedAnalysisState = tag_generator_app.invoke(stage2_input)
    
    print(f"‚úÖ Stage 2 Complete. Final keywords curated.")

    # ----------------------------------------------------------------
    # FINAL ASSEMBLY
    # ----------------------------------------------------------------
    
    # The final output is assembled from data generated by both agents:
    # - Summaries/Docs/Files from Stage 1
    # - Curated Keywords from Stage 2
    
    final_result = {
        "project_summary": stage1_output.get("summaries", {}).get("README.md", "No README available"),
        "file_summaries": stage1_output.get("summaries"),
        "missing_documentation": stage1_output.get("missing_docs"),
        "keywords": stage2_output.get("keywords", []), # Take the result from the Tag Graph
        "file_structure": stage1_output.get("final_output", {}).get("file_structure", {}),
        "suggested_tags": stage2_output.get("keywords", [])[:5]
    }
    
    return final_result

# ----------------------------------------------------------------
# Main Execution Block
# ----------------------------------------------------------------
if __name__ == "__main__":
    
    # NOTE: Ensure the GROQ_API_KEY environment variable is set.
    
    repo_to_analyze = "https://github.com/roshan9419/LearnEd_E-learning_Website"
    
    try:
        final_analysis = run_assembly_line_analysis(repo_to_analyze)
        
        print("\n\n=============== FINAL REPORT ===============")
        print(json.dumps(final_analysis, indent=2))
        print("============================================")

    except Exception as e:
        print(f"\n‚ùå An error occurred during the assembly line execution: {e}")
        # Detailed traceback recommended for debugging production code